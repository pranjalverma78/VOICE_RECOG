{"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":256618,"sourceType":"datasetVersion","datasetId":107620},{"sourceId":529602,"sourceType":"datasetVersion","datasetId":251883},{"sourceId":639622,"sourceType":"datasetVersion","datasetId":316368},{"sourceId":7929891,"sourceType":"datasetVersion","datasetId":4660900}],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"papermill":{"default_parameters":{},"duration":15751.335657,"end_time":"2024-05-24T01:27:10.693636","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-05-23T21:04:39.357979","version":"2.5.0"}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"code","source":"# # This Python 3 environment comes with many helpful analytics libraries installed\n# # It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# # For example, here's several helpful packages to load\n\n# import numpy as np # linear algebra\n# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# # Input data files are available in the read-only \"../input/\" directory\n# # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# # You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# # You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":0.016992,"end_time":"2024-05-23T21:04:42.522413","exception":false,"start_time":"2024-05-23T21:04:42.505421","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-09-01T13:27:00.733286Z","iopub.execute_input":"2024-09-01T13:27:00.733681Z","iopub.status.idle":"2024-09-01T13:27:00.738999Z","shell.execute_reply.started":"2024-09-01T13:27:00.733641Z","shell.execute_reply":"2024-09-01T13:27:00.738169Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# !pip install tqdm pandas wandb librosa numpy torchmetrics torch\n# !pip3 install tqdm","metadata":{"execution":{"iopub.status.busy":"2024-09-01T13:27:00.740798Z","iopub.execute_input":"2024-09-01T13:27:00.741116Z","iopub.status.idle":"2024-09-01T13:27:00.752319Z","shell.execute_reply.started":"2024-09-01T13:27:00.741092Z","shell.execute_reply":"2024-09-01T13:27:00.751015Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import warnings\nfrom tqdm import tqdm\nimport os\nfrom pathlib import Path\nimport math\nimport pandas as pd\n\nimport wandb\n\nimport librosa\n\nimport numpy as np\n\nimport torchmetrics\nimport torchmetrics.classification\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom torch.optim import lr_scheduler\nfrom torch.optim.lr_scheduler import LambdaLR\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"papermill":{"duration":10.536445,"end_time":"2024-05-23T21:04:53.065941","exception":false,"start_time":"2024-05-23T21:04:42.529496","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-09-01T13:27:00.753698Z","iopub.execute_input":"2024-09-01T13:27:00.754073Z","iopub.status.idle":"2024-09-01T13:27:06.796856Z","shell.execute_reply.started":"2024-09-01T13:27:00.754043Z","shell.execute_reply":"2024-09-01T13:27:06.796010Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preparing the dataset\n\nThis notebook will use the global audio datasets for the emotion recognition model. This will give us the relative success of the model compared to pre-existing models","metadata":{"papermill":{"duration":0.006334,"end_time":"2024-05-23T21:04:53.079031","exception":false,"start_time":"2024-05-23T21:04:53.072697","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"## Create csv file saving the location of the global datset.\n\nWe are using the TESS, RAV, SAVEE and CREMA datasets","metadata":{"papermill":{"duration":0.00621,"end_time":"2024-05-23T21:04:53.091649","exception":false,"start_time":"2024-05-23T21:04:53.085439","status":"completed"},"tags":[]}},{"cell_type":"code","source":"#for dirname, _, filenames in os.walk('/kaggle/input'):\n#    for filename in filenames:\n#        print(os.path.join(dirname, filename))\n\nTESS = \"/kaggle/input/toronto-emotional-speech-set-tess/tess toronto emotional speech set data/TESS Toronto emotional speech set data/\"\nRAV = \"/kaggle/input/ravdess-emotional-speech-audio/audio_speech_actors_01-24/\"\n# SAVEE = \"/kaggle/input/surrey-audiovisual-expressed-emotion-savee/ALL/\"\n# CREMA = \"/kaggle/input/cremad/AudioWAV/\"\n\n\n# Run one example \ndir_list = os.listdir(RAV)\ndir_list[0:5]","metadata":{"papermill":{"duration":0.028784,"end_time":"2024-05-23T21:04:53.127217","exception":false,"start_time":"2024-05-23T21:04:53.098433","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-09-01T13:27:06.799293Z","iopub.execute_input":"2024-09-01T13:27:06.799865Z","iopub.status.idle":"2024-09-01T13:27:06.818919Z","shell.execute_reply.started":"2024-09-01T13:27:06.799828Z","shell.execute_reply":"2024-09-01T13:27:06.818073Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"['Actor_02', 'Actor_17', 'Actor_05', 'Actor_16', 'Actor_21']"},"metadata":{}}]},{"cell_type":"markdown","source":"RAVDESS editiong","metadata":{"papermill":{"duration":0.006892,"end_time":"2024-05-23T21:04:53.141110","exception":false,"start_time":"2024-05-23T21:04:53.134218","status":"completed"},"tags":[]}},{"cell_type":"code","source":"dir_list = os.listdir(RAV)\ndir_list.sort()\n\nemotion = []\ngender = []\npath = []\nfor i in dir_list:\n    fname = os.listdir(RAV + i)\n    for f in fname:\n        part = f.split('.')[0].split('-')\n        emotion.append(int(part[2]))\n        temp = int(part[6])\n        if temp%2 == 0:\n            temp = \"female\"\n        else:\n            temp = \"male\"\n        gender.append(temp)\n        path.append(RAV + i + '/' + f)\n\n        \nRAV_df = pd.DataFrame(emotion)\nRAV_df = RAV_df.replace({1:'neutral', 2:'neutral', 3:'happy', 4:'sad', 5:'angry', 6:'fear', 7:'disgust', 8:'surprise'})\nRAV_df = pd.concat([pd.DataFrame(gender),RAV_df],axis=1)\nRAV_df.columns = ['gender','emotion']\nRAV_df['labels'] =RAV_df.gender + '_' + RAV_df.emotion\nRAV_df['source'] = 'RAVDESS'  \nRAV_df = pd.concat([RAV_df,pd.DataFrame(path, columns = ['path'])],axis=1)\nRAV_df = RAV_df.drop(['gender', 'emotion'], axis=1)\nRAV_df.labels.value_counts()","metadata":{"papermill":{"duration":0.569335,"end_time":"2024-05-23T21:04:53.717565","exception":false,"start_time":"2024-05-23T21:04:53.148230","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-09-01T13:27:06.820006Z","iopub.execute_input":"2024-09-01T13:27:06.820319Z","iopub.status.idle":"2024-09-01T13:27:07.480964Z","shell.execute_reply.started":"2024-09-01T13:27:06.820293Z","shell.execute_reply":"2024-09-01T13:27:07.480032Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"labels\nmale_neutral       144\nfemale_neutral     144\nmale_fear           96\nmale_happy          96\nmale_disgust        96\nmale_sad            96\nmale_angry          96\nmale_surprise       96\nfemale_surprise     96\nfemale_disgust      96\nfemale_fear         96\nfemale_sad          96\nfemale_happy        96\nfemale_angry        96\nName: count, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"dir_list = os.listdir(TESS)\ndir_list.sort()\n\npath = []\nemotion = []\n\nfor i in dir_list:\n    fname = os.listdir(TESS + i)\n    for f in fname:\n        if i == 'OAF_angry' or i == 'YAF_angry':\n            emotion.append('female_angry')\n        elif i == 'OAF_disgust' or i == 'YAF_disgust':\n            emotion.append('female_disgust')\n        elif i == 'OAF_Fear' or i == 'YAF_fear':\n            emotion.append('female_fear')\n        elif i == 'OAF_happy' or i == 'YAF_happy':\n            emotion.append('female_happy')\n        elif i == 'OAF_neutral' or i == 'YAF_neutral':\n            emotion.append('female_neutral')                                \n        elif i == 'OAF_Pleasant_surprise' or i == 'YAF_pleasant_surprised':\n            emotion.append('female_surprise')               \n        elif i == 'OAF_Sad' or i == 'YAF_sad':\n            emotion.append('female_sad')\n        else:\n            emotion.append('Unknown')\n        path.append(TESS + i + \"/\" + f)\n\nTESS_df = pd.DataFrame(emotion, columns = ['labels'])\nTESS_df['source'] = 'TESS'\nTESS_df = pd.concat([TESS_df,pd.DataFrame(path, columns = ['path'])],axis=1)\nTESS_df.labels.value_counts()","metadata":{"papermill":{"duration":0.991523,"end_time":"2024-05-23T21:04:54.716641","exception":false,"start_time":"2024-05-23T21:04:53.725118","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-09-01T13:27:07.482112Z","iopub.execute_input":"2024-09-01T13:27:07.482510Z","iopub.status.idle":"2024-09-01T13:27:08.712336Z","shell.execute_reply.started":"2024-09-01T13:27:07.482482Z","shell.execute_reply":"2024-09-01T13:27:08.711277Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"labels\nfemale_fear        400\nfemale_surprise    400\nfemale_sad         400\nfemale_angry       400\nfemale_disgust     400\nfemale_happy       400\nfemale_neutral     400\nName: count, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"df = pd.concat([RAV_df, TESS_df], axis = 0)\nprint(df.labels.value_counts())\ndf.head()\ndf.to_csv(\"Data_path.csv\",index=False)","metadata":{"papermill":{"duration":0.061873,"end_time":"2024-05-23T21:04:54.786162","exception":false,"start_time":"2024-05-23T21:04:54.724289","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-09-01T13:27:08.713587Z","iopub.execute_input":"2024-09-01T13:27:08.713938Z","iopub.status.idle":"2024-09-01T13:27:08.757183Z","shell.execute_reply.started":"2024-09-01T13:27:08.713905Z","shell.execute_reply":"2024-09-01T13:27:08.756287Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"labels\nfemale_neutral     544\nfemale_sad         496\nfemale_fear        496\nfemale_disgust     496\nfemale_surprise    496\nfemale_happy       496\nfemale_angry       496\nmale_neutral       144\nmale_angry          96\nmale_fear           96\nmale_sad            96\nmale_disgust        96\nmale_happy          96\nmale_surprise       96\nName: count, dtype: int64\n","output_type":"stream"}]},{"cell_type":"code","source":"ref = pd.read_csv(\"/kaggle/working/Data_path.csv\")","metadata":{"papermill":{"duration":0.034733,"end_time":"2024-05-23T21:04:54.828632","exception":false,"start_time":"2024-05-23T21:04:54.793899","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-09-01T13:27:08.758248Z","iopub.execute_input":"2024-09-01T13:27:08.758548Z","iopub.status.idle":"2024-09-01T13:27:08.777982Z","shell.execute_reply.started":"2024-09-01T13:27:08.758523Z","shell.execute_reply":"2024-09-01T13:27:08.777275Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"class AudioDataset(Dataset):\n    def __init__(self, data, seq_len, d_model, augment=False):\n        super().__init__()\n        self.seq_len = seq_len\n        self.d_model = d_model\n        self.data = data\n        self.augment = augment\n        self.num_augments = 1  # Default number of augmentations (original only)\n\n        if self.augment:\n            self.num_augments = 4   # Add augment parameter\n\n        self.class_map = {\n            \"female_neutral\": 0, \"female_surprise\": 1, \"female_disgust\": 2, \n            \"female_fear\": 3, \"female_sad\": 4, \"female_happy\": 5, \"female_angry\": 6, \n            \"male_neutral\": 7, \"male_sad\": 8, \"male_fear\": 9, \"male_happy\": 10, \n            \"male_disgust\": 11, \"male_angry\": 12, \"male_surprise\": 13\n        }\n        self.label_map = {0: \"female_neutral\", 1: \"female_surprise\", 2: \"female_disgust\", \n                          3: \"female_fear\", 4: \"female_sad\", 5: \"female_happy\", 6: \"female_angry\", \n                          7: \"male_neutral\", 8: \"male_sad\", 9: \"male_fear\", 10: \"male_happy\",\n                          11: \"male_disgust\", 12: \"male_angry\", 13: \"male_surprise\"}\n\n    @staticmethod\n    def extract_audio_features(signal, sample_rate=44100):\n        # MFCC\n        mfccs = librosa.feature.mfcc(y=signal, n_mfcc=13, sr=sample_rate)\n        delta_mfccs = librosa.feature.delta(mfccs)\n        delta2_mfccs = librosa.feature.delta(mfccs, order=2)\n        # Root Mean Square Energy\n#         root_mean_out = librosa.feature.rms(y=signal)\n        mfccs_features = np.concatenate((mfccs, delta_mfccs, delta2_mfccs))\n        return mfccs_features\n\n    def __len__(self):\n        return len(self.data) * self.num_augments\n\n    \n    def __getitem__(self, idx):\n        # Determine which original sample and which augmentation to use\n        original_idx = idx // self.num_augments\n        augment_idx = idx % self.num_augments\n\n        file_path, emotion = self.data[original_idx]\n        signal, sr = librosa.load(file_path)\n\n        # Create a list of signals (including augmentations)\n        signals = [signal]  # Start with the original signal\n\n        if self.augment:\n            noisy_signal = self.add_noise(signal)\n            stretch_signal = self.stretch_process(signal)\n            pitch_signal = self.pitch_process(signal, sr)\n            signals.extend([noisy_signal, stretch_signal, pitch_signal])\n\n        # Select the signal based on the augmentation index\n        sig = signals[augment_idx]\n        inp = AudioDataset.extract_audio_features(sig, sr)\n\n        # Padding\n        padding = self.seq_len - inp.shape[1]\n        if padding < 0:\n            raise ValueError(\"Audio is too long\")\n        \n        inp = torch.cat(\n            [\n                torch.tensor(inp, dtype=torch.float32),\n                torch.zeros((self.d_model, padding), dtype=torch.float32)\n            ], 1\n        )\n\n        label = torch.zeros(14)\n        label[self.class_map[emotion]] = 1\n\n        sample = {\n            \"label\": label,\n            \"file_path\": file_path,\n            \"input\": inp.T,\n            \"emotion\": emotion,\n            \"class\": self.class_map[emotion]\n        }\n        \n        return sample\n\n    \n    @staticmethod\n    def add_noise(signal, noise_level=0.005):\n        noise = np.random.randn(len(signal))\n        signal_noisy = signal + noise_level * noise\n        return signal_noisy\n\n    @staticmethod\n    def stretch_process(signal, rate=1.2):\n        return librosa.effects.time_stretch(signal, rate=rate)\n\n    @staticmethod\n    def pitch_process(signal, sr, n_steps=4):\n        return librosa.effects.pitch_shift(signal, sr=sr, n_steps=n_steps)\n\n    \n    \ndef causal_mask(size):\n    mask = torch.triu(torch.ones((1, size, size)), diagonal=1).type(torch.int)\n    return mask == 0\n\n\ndef get_label(vector):\n    # class_map = {\"anger\" : 0, \"sadness\": 1, \"fear\": 2, \"happy\": 3, \"neutral\": 4, \"surprise\": 5, \"sarcastic\": 6, \"disgust\": 7}\n    label_map = {0: \"anger\", 1: \"sadness\", 2: \"fear\", 3: \"happy\", 4: \"neutral\", 5: \"surprise\", 6: \"sarcastic\", 7: \"disgust\"}\n\n    return label_map[np.argmax(vector)]","metadata":{"papermill":{"duration":0.028288,"end_time":"2024-05-23T21:04:54.865534","exception":false,"start_time":"2024-05-23T21:04:54.837246","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-09-01T13:27:08.780513Z","iopub.execute_input":"2024-09-01T13:27:08.780766Z","iopub.status.idle":"2024-09-01T13:27:08.800960Z","shell.execute_reply.started":"2024-09-01T13:27:08.780746Z","shell.execute_reply":"2024-09-01T13:27:08.800087Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"class LayerNormalization(nn.Module):\n\n    def __init__(self, features: int, eps:float=10**-6) -> None:\n        super().__init__()\n        self.eps = eps\n        self.alpha = nn.Parameter(torch.ones(features)) # alpha is a learnable parameter\n        self.bias = nn.Parameter(torch.zeros(features)) # bias is a learnable parameter\n\n    def forward(self, x):\n        # x: (batch, seq_len, hidden_size)\n         # Keep the dimension for broadcasting\n        mean = x.mean(dim = -1, keepdim = True) # (batch, seq_len, 1)\n        # Keep the dimension for broadcasting\n        std = x.std(dim = -1, keepdim = True) # (batch, seq_len, 1)\n        # eps is to prevent dividing by zero or when std is very small\n        return self.alpha * (x - mean) / (std + self.eps) + self.bias\n\nclass FeedForwardBlock(nn.Module):\n\n    def __init__(self, d_model: int, d_ff: int, dropout: float) -> None:\n        super().__init__()\n        self.dropout_1 = nn.Dropout(dropout)\n        self.dropout_2 = nn.Dropout(dropout)\n        self.dropout_3 = nn.Dropout(dropout)\n        self.dropout_4 = nn.Dropout(dropout)\n        \n        # linear model\n        self.linear_1 = nn.Linear(d_model, d_ff) \n        self.linear_2 = nn.Linear(d_ff, d_ff)\n        self.linear_3 = nn.Linear(d_ff, d_ff)\n        self.linear_4 = nn.Linear(d_ff, d_ff)        \n        self.linear_5 = nn.Linear(d_ff, d_model) \n        \n        # cnn model\n        # self.conv_1 = nn.Conv2d(1, 8, kernel_size=(2, 2), padding=1, stride=1)\n        # self.conv_2 = nn.Conv2d(8, 64, kernel_size=(2, 2), padding=1, stride=1)\n        # self.conv_3 = nn.Conv2d(64, 128, kernel_size=(2, 2), padding=1, stride=1)\n        # self.conv_4 = nn.Conv2d(128, 64, kernel_size=(2, 2), padding=1, stride=1)\n        # self.conv_5 = nn.Conv2d(64, 8, kernel_size=(2, 2), padding=1, stride=1)\n        # self.conv_6 = nn.Conv2d(8, 1, kernel_size=(2, 2), padding=1, stride=1)\n\n    def forward(self, x):\n        # (batch, seq_len, d_model) --> (batch, seq_len, d_ff) --> (batch, seq_len, d_model)\n        x = self.linear_1(x)\n        x = self.linear_2(self.dropout_1(torch.relu(x)))\n        x = self.linear_3(self.dropout_2(torch.relu(x)))\n        x = self.linear_4(self.dropout_3(torch.relu(x)))\n        x = self.linear_5(self.dropout_4(torch.relu(x)))\n        \n        # convo forward \n        # (batch, seq_len, d_model) --> (batch, 1, seq_len, d_model)\n        # x = x.unsqeeze(1)\n        \n        return x\n\n    \nclass PositionalEncoding(nn.Module):\n\n    def __init__(self, d_model: int, seq_len: int, dropout: float) -> None:\n        super().__init__()\n        self.d_model = d_model\n        self.seq_len = seq_len\n        self.dropout = nn.Dropout(dropout)\n        # Create a matrix of shape (seq_len, d_model)\n        pe = torch.zeros(seq_len, d_model)\n        # Create a vector of shape (seq_len)\n        position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1) # (seq_len, 1)\n        # Create a vector of shape (d_model)\n        sin_div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)) # (d_model / 2)\n        cos_div_term = torch.exp(torch.arange(1, d_model, 2).float() * (-math.log(10000.0) / d_model)) # (d_model / 2)\n        # Apply sine to even indices\n        pe[:, 0::2] = torch.sin(position * sin_div_term) # sin(position * (10000 ** (2i / d_model))\n        # Apply cosine to odd indices\n        pe[:, 1::2] = torch.cos(position * cos_div_term) # cos(position * (10000 ** (2i / d_model))\n        # Add a batch dimension to the positional encoding\n        pe = pe.unsqueeze(0) # (1, seq_len, d_model)\n        # Register the positional encoding as a buffer\n        self.register_buffer('pe', pe)\n\n    def forward(self, x):\n        x = x + (self.pe[:, :x.shape[1], :]).requires_grad_(False) # (batch, seq_len, d_model)\n        return self.dropout(x)\n\nclass ResidualConnection(nn.Module):\n    \n        def __init__(self, features: int, dropout: float) -> None:\n            super().__init__()\n            self.dropout = nn.Dropout(dropout)\n            self.norm = LayerNormalization(features)\n    \n        def forward(self, x, sublayer):\n            return x + self.dropout(sublayer(self.norm(x)))\n\nclass MultiHeadAttentionBlock(nn.Module):\n\n    def __init__(self, d_model: int, h: int, dropout: float) -> None:\n        super().__init__()\n        self.d_model = d_model # Embedding vector size\n        self.h = h # Number of heads\n        # Make sure d_model is divisible by h\n        assert d_model % h == 0, \"d_model is not divisible by h\"\n\n        self.d_k = d_model // h # Dimension of vector seen by each head\n        self.w_q = nn.Linear(d_model, d_model, bias=False) # Wq\n        self.w_k = nn.Linear(d_model, d_model, bias=False) # Wk\n        self.w_v = nn.Linear(d_model, d_model, bias=False) # Wv\n        self.w_o = nn.Linear(d_model, d_model, bias=False) # Wo\n        self.dropout = nn.Dropout(dropout)\n\n    @staticmethod\n    def attention(query, key, value, mask = None , dropout: nn.Dropout = None):\n        d_k = query.shape[-1]\n        # Just apply the formula from the paper\n        # (batch, h, seq_len, d_k) --> (batch, h, seq_len, seq_len)\n        attention_scores = (query @ key.transpose(-2, -1)) / math.sqrt(d_k)\n        if mask is not None:\n            # Write a very low value (indicating -inf) to the positions where mask == 0\n            attention_scores.masked_fill_(mask == 0, -1e9)\n        attention_scores = attention_scores.softmax(dim=-1) # (batch, h, seq_len, seq_len) # Apply softmax\n        if dropout is not None:\n            attention_scores = dropout(attention_scores)\n        # (batch, h, seq_len, seq_len) --> (batch, h, seq_len, d_k)\n        # return attention scores which can be used for visualization\n        return (attention_scores @ value), attention_scores\n\n    def forward(self, q, k, v, mask = None):\n        query = self.w_q(q) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n        key = self.w_k(k) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n        value = self.w_v(v) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n\n        # (batch, seq_len, d_model) --> (batch, seq_len, h, d_k) --> (batch, h, seq_len, d_k)\n        query = query.view(query.shape[0], query.shape[1], self.h, self.d_k).transpose(1, 2)\n        key = key.view(key.shape[0], key.shape[1], self.h, self.d_k).transpose(1, 2)\n        value = value.view(value.shape[0], value.shape[1], self.h, self.d_k).transpose(1, 2)\n\n        # Calculate attention\n        x, self.attention_scores = MultiHeadAttentionBlock.attention(query, key, value, mask, self.dropout)\n        \n        # Combine all the heads together\n        # (batch, h, seq_len, d_k) --> (batch, seq_len, h, d_k) --> (batch, seq_len, d_model)\n        x = x.transpose(1, 2).contiguous().view(x.shape[0], -1, self.h * self.d_k)\n\n        # Multiply by Wo\n        # (batch, seq_len, d_model) --> (batch, seq_len, d_model)  \n        return self.w_o(x)\n\nclass EncoderBlock(nn.Module):\n\n    def __init__(self, features: int, self_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float) -> None:\n        super().__init__()\n        self.self_attention_block = self_attention_block\n        self.feed_forward_block = feed_forward_block\n        self.residual_connections = nn.ModuleList([ResidualConnection(features, dropout) for _ in range(2)])\n\n    def forward(self, x, src_mask = None):\n        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, src_mask))\n        x = self.residual_connections[1](x, self.feed_forward_block)\n        return x\n    \n    \nclass Encoder(nn.Module):\n\n    def __init__(self, features: int, layers: nn.ModuleList) -> None:\n        super().__init__()\n        self.layers = layers\n        self.norm = LayerNormalization(features)\n\n    def forward(self, x, mask = None):\n        for layer in self.layers:\n            x = layer(x, mask)\n        return self.norm(x)\n\n\nclass ProjectionLayer(nn.Module):\n\n    def __init__(self, seq_len: int, d_model: int, d_ff: int, num_of_labels: int, dropout: float) -> None:\n        super().__init__()\n        self.dropout_1 = nn.Dropout(dropout)\n        self.dropout_2 = nn.Dropout(dropout)\n        self.dropout_3 = nn.Dropout(dropout)\n        self.dropout_4 = nn.Dropout(dropout)\n        self.dropout_5 = nn.Dropout(dropout)\n        self.dropout_6 = nn.Dropout(dropout)\n        \n        self.proj = nn.Linear(d_ff, num_of_labels)\n\n#         self.proj = nn.Linear(512, num_of_labels)\n        # dense network\n#         self.linear_1 = nn.Linear(d_model, d_ff)\n#         self.linear_2 = nn.Linear(d_ff, d_ff)\n#         self.linear_3 = nn.Linear(d_ff, d_model)\n#         self.linear_4 = nn.Linear(d_model*seq_len, d_ff)\n#         self.linear_5 = nn.Linear(d_ff, d_ff)\n#         self.linear_6 = nn.Linear(d_ff, d_ff)\n        \n        \n        # cnn model\n        # self.conv_1 = nn.Conv2d(1, 8, padding_mode='replicate')\n        # self.conv_2 = nn.Conv2d(8, 64, padding_mode='replicate')\n        # self.conv_3 = nn.Conv2d(64, 8, padding_mode='replicate')\n        # self.conv_4 = nn.Conv2d(8, 1, padding_mode='replicate')\n        self.conv1 = nn.Conv2d(in_channels=1, out_channels=8, kernel_size=3, stride=1, padding=1)\n        self.batchnorm_1 = nn.BatchNorm2d(8)\n        self.pool_1 = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n        self.conv2 = nn.Conv2d(in_channels=8, out_channels=64, kernel_size=3, stride=1, padding=1)\n        self.batchnorm_2 = nn.BatchNorm2d(64)\n        self.pool_2 = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)\n        self.batchnorm_3 = nn.BatchNorm2d(128)\n        self.pool_3 = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n        self.conv4 = nn.Conv2d(in_channels=128, out_channels=d_ff, kernel_size=(75, 4), stride=1, padding=0)\n        self.batchnorm_4 = nn.BatchNorm2d(d_ff)\n\n\n    def forward(self, x) -> None:\n        # (batch, seq_len, d_model) --> (batch, num_of_labels)\n#         x = self.linear_1(x)\n#         x = self.linear_2(self.dropout_1(torch.relu(x)))\n#         x = self.linear_3(self.dropout_2(torch.relu(x)))\n#         x = torch.flatten(x,1)\n#         x = self.linear_4(self.dropout_3(torch.relu(x)))\n#         x = self.linear_5(self.dropout_4(torch.relu(x)))\n#         x = self.linear_6(self.dropout_5(torch.relu(x)))\n#         x = self.proj(self.dropout_6(torch.relu(x)))\n        \n        # convo forward \n        # (batch, seq_len, d_model) --> (batch, 1, seq_len, d_model)\n        x = torch.unsqueeze(x, 1)\n        x = self.dropout_1(self.pool_1(torch.relu(self.batchnorm_1(self.conv1(x)))))\n        x = self.dropout_2(self.pool_2(torch.relu(self.batchnorm_2(self.conv2(x)))))\n        x = self.dropout_3(self.pool_3(torch.relu(self.batchnorm_3(self.conv3(x)))))\n        x = self.dropout_4(torch.relu(self.batchnorm_4(self.conv4(x))))\n        \n        x = torch.flatten(x, 1)\n        x = self.proj(x)\n        \n        return x\n\n\n\n    \nclass Transformer(nn.Module):\n\n    def __init__(self, encoder: Encoder, inp_pos: PositionalEncoding, projection_layer: ProjectionLayer) -> None:\n        super().__init__()\n        self.encoder = encoder\n        self.inp_pos = inp_pos\n        self.projection_layer = projection_layer\n\n    def encode(self, inp, inp_mask = None):\n        # (batch, seq_len, d_model)\n        inp = self.inp_pos(inp)\n        return self.encoder(inp, inp_mask)\n    \n    def project(self, x):\n        # (batch, num_of_label)\n        return self.projection_layer(x)\n    \n\n\ndef build_transformer(seq_len: int, num_of_labels: int, d_model: int=39, N: int=5, h: int=3, dropout: float=0.1, d_ff: int=256) -> Transformer:\n    # Create the embedding layers\n\n    # Create the positional encoding layers\n    inp_pos = PositionalEncoding(d_model, seq_len, dropout)\n    \n    # Create the encoder blocks\n    encoder_blocks = []\n    for _ in range(N):\n        encoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n        encoder_block = EncoderBlock(d_model, encoder_self_attention_block, feed_forward_block, dropout)\n        encoder_blocks.append(encoder_block)\n\n    \n    # Create the encoder and decoder\n    encoder = Encoder(d_model, nn.ModuleList(encoder_blocks))\n    \n    # Create the projection layer\n    projection_layer = ProjectionLayer(seq_len, d_model, d_ff, num_of_labels, dropout)\n    \n    # Create the transformer\n    transformer = Transformer(encoder, inp_pos, projection_layer)\n    transformer.to(device)\n    \n    # Initialize the parameters\n    for p in transformer.parameters():\n        if p.dim() > 1:\n            nn.init.xavier_uniform_(p)\n    \n    return transformer\n\n\nclass SquareRootScheduler:\n    def __init__(self, lr=0.1):\n        self.lr = lr\n\n    def __call__(self, num_update):\n        return self.lr * pow(num_update + 1.0, -0.5)","metadata":{"papermill":{"duration":0.074374,"end_time":"2024-05-23T21:04:54.947498","exception":false,"start_time":"2024-05-23T21:04:54.873124","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-09-01T13:27:08.802424Z","iopub.execute_input":"2024-09-01T13:27:08.802915Z","iopub.status.idle":"2024-09-01T13:27:08.856321Z","shell.execute_reply.started":"2024-09-01T13:27:08.802884Z","shell.execute_reply":"2024-09-01T13:27:08.855587Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"def get_config():\n    return {\n        \"dataset_root_location\": \"/kaggle/working/\",\n        \"df_location\": \"Data_path.csv\",\n        \"num_of_labels\": 14,\n        \"batch_size\": 16,\n        \"num_epochs\": 50,\n        \"lr\": 0.1,\n        \"seq_len\": 600,\n        \"d_model\": 39,\n        \"datasource\": \"Global_dataset_ravdess_tess\",\n        \"model_folder\": \"weights\",\n        \"model_basename\": \"tmodel_\",\n        \"preload\": \"latest\",\n        \"experiment_name\": \"runs/tmodel\"\n    }\n\ndef get_weights_file_path(config, epoch: str):\n    model_folder = f\"{config['datasource']}_{config['model_folder']}\"\n    model_filename = f\"{config['model_basename']}{epoch}.pt\"\n    return str(Path('.') / model_folder / model_filename)\n\n# Find the latest weights file in the weights folder\ndef latest_weights_file_path(config):\n    model_folder = f\"{config['datasource']}_{config['model_folder']}\"\n    model_filename = f\"{config['model_basename']}*\"\n    weights_files = list(Path(model_folder).glob(model_filename))\n    if len(weights_files) == 0:\n        return None\n    weights_files.sort()\n    return str(weights_files[-1])\n","metadata":{"papermill":{"duration":0.020451,"end_time":"2024-05-23T21:04:54.976100","exception":false,"start_time":"2024-05-23T21:04:54.955649","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-09-01T13:27:08.857195Z","iopub.execute_input":"2024-09-01T13:27:08.857475Z","iopub.status.idle":"2024-09-01T13:27:08.871191Z","shell.execute_reply.started":"2024-09-01T13:27:08.857452Z","shell.execute_reply":"2024-09-01T13:27:08.870310Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# def run_validation(model, validation_ds, config, device, global_step):\n#     model.eval()\n#     count = 0\n\n#     true_classes = []\n#     pred_classes = []\n\n#     # try:\n#     #     # get the console window width\n#     #     with os.popen('stty size', 'r') as console:\n#     #         _, console_width = console.read().split()\n#     #         console_width = int(console_width)\n#     # except:\n#     #     # If we can't get the console width, use 80 as default\n#     #     console_width = 80\n\n#     with torch.no_grad():\n#         for batch in validation_ds:\n#             count += 1\n#             inp = batch[\"input\"].to(device) # (b, d_model, seq_len)\n#             inp_mask = None\n\n#             # check that the batch size is 1\n#             assert inp.size(\n#                 0) == 1, \"Batch size must be 1 for validation\"\n\n#             out = model.encode(inp)\n#             out = model.project(out).squeeze(0)\n#             # pred_class = (np.zeros_like(out) == 0)[np.argmax(out)] = 1\n\n#             # storing results\n#             pred_classes.append(torch.argmax(out).item())\n#             true_classes.append(torch.argmax(batch[\"label\"].squeeze(0)).item())\n#         true_classes = torch.tensor(true_classes)\n#         pred_classes = torch.tensor(pred_classes)\n#         # Print the source, target and model output\n#         # print_msg('-'*console_width)\n#         # print_msg(f\"{f'TARGET: ':>12}{label}\")3\n#         # print_msg(f\"{f'PREDICTED: ':>12}{pred_out}\")\n#         # print_msg('-'*console_width)\n    \n    \n#     # Evaluate the character error rate\n#     # Compute the char error rate \n#     # metric = torchmetrics.classification.MulticlassConfusionMatrix(num_classes=config[\"num_of_labels\"])\n#     # cer = metric(pred_classes, true_classes)\n#     # wandb.log({'validation/cer': cer, 'global_step': global_step})\n\n#     # Compute confusion matrix\n#     confusion_matrix = torchmetrics.ConfusionMatrix(num_classes=config[\"num_of_labels\"], task=\"multiclass\")\n#     confusion_matrix(pred_classes, true_classes)\n\n#     # Compute F1-score\n#     f1_score = torchmetrics.classification.MulticlassF1Score(num_classes=config[\"num_of_labels\"])\n#     f1_score(pred_classes, true_classes)\n\n#     # Compute accuracy\n#     accuracy = torchmetrics.classification.MulticlassAccuracy(num_classes=config[\"num_of_labels\"])\n#     accuracy(pred_classes, true_classes) \n\n#     # Compute recall\n#     recall = torchmetrics.classification.MulticlassRecall(num_classes=config[\"num_of_labels\"])\n#     recall(pred_classes, true_classes)   \n\n#     # Compute precision\n#     precision = torchmetrics.classification.MulticlassPrecision(num_classes=config[\"num_of_labels\"])\n#     precision(pred_classes, true_classes)\n\n#     # Log metrics to WandB\n#     wandb.log({\"validation/confusion_matrix\": confusion_matrix.compute(),\n#                \"validation/accuracy\": accuracy.compute(), \n#                \"validation/recall\": recall.compute(), \n#                \"validation/precision\": precision.compute(), \n#                \"validation/f1_score\": f1_score.compute(),\n#                'global_step': global_step})\n\n# def get_models(config,num_models=3):\n#     models=[]\n#     for _ in range(numsm_models):\n#         model = build_transformer(config[\"seq_len\"], config['num_of_labels'], d_model=config['d_model'])\n#         models.append(model)\n        \n#     return models\n\n# def train_model(config, num_models=3):\n#     # Define the device\n#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n#     print(\"Using device:\", device)\n\n#     # Make sure the weights folder exists\n#     Path(config['model_folder']).mkdir(parents=True, exist_ok=True)\n\n#     train_dataloader, val_dataloader = get_ds(config)\n# #     model = get_model(config).to(device)\n#     models = get_models(config,num_models)\n#     models = [model.to(device) for model in models]\n    \n# #     optimizer = torch.optim.SGD(model.parameters(), lr=config['lr'])\n#     optimizers = [torch.optim.SGD(model.parameters(), lr=config['lr']) for model in models]\n    \n#     scheduler = SquareRootScheduler(lr=config['lr'])\n\n#     # If the user specified a model to preload before training, load it\n#     initial_epoch = 0\n#     global_step = 0\n#     if config['preload']:\n#         model_filename = latest_weights_file_path(config)\n#         print(f'Preloading model {model_filename}')\n#         state = torch.load(model_filename)\n#         model.load_state_dict(state['model_state_dict'])\n#         initial_epoch = state['epoch'] + 1\n#         optimizer.load_state_dict(state['optimizer_state_dict'])\n#         global_step = state['global_step']\n#         del state\n\n#     loss_fn = nn.CrossEntropyLoss(label_smoothing=0.1).to(device)\n\n#     # define our custom x axis metric\n#     wandb.define_metric(\"global_step\")\n#     # define which metrics will be plotted against it\n#     wandb.define_metric(\"validation/*\", step_metric=\"global_step\")\n#     wandb.define_metric(\"train/*\", step_metric=\"global_step\")\n\n#     for epoch in range(initial_epoch, initial_epoch+config['num_epochs']):\n#         torch.cuda.empty_cache()\n# #         model.train()\n#         batch_iterator = tqdm(train_dataloader, desc=f\"Processing Epoch {epoch:02d}\")\n#         for batch in batch_iterator:\n            \n#             # moved here\n#             model.train()\n#             optimizer.zero_grad()\n\n#             input = batch['input'].to(device) # (b, seq_len, d_model)\n\n#             # Run the tensors through the encoder, decoder and the projection layer\n#             encoder_output = model.encode(input) # (B, seq_len, d_model)\n#             proj_output = model.project(encoder_output) # (B, num_of_labels)\n\n#             # Compare the output with the label\n#             label = batch['label'].to(device) # (B, num_of_labels)\n\n#             # Compute the loss using a simple cross entropy\n#             loss = loss_fn(proj_output, label)\n#             batch_iterator.set_postfix({\"loss\": f\"{loss.item():6.3f}\"})\n\n#             # Log the loss\n#             wandb.log({'train/loss': loss.item(), 'global_step': global_step})\n\n#             # Backpropagate the loss\n#             loss.backward()\n\n#             # Update the weights\n#             optimizer.step()\n# #             optimizer.zero_grad(set_to_none=True)\n\n#         global_step += 1\n        \n#         # updating scheduler\n#         if scheduler:\n#             if scheduler.__module__ == lr_scheduler.__name__:\n#                 # Using PyTorch In-Built scheduler\n#                 scheduler.step()\n#             else:\n#                 # Using custom defined scheduler\n#                 for param_group in optimizer.param_groups:\n#                     param_group['lr'] = scheduler(epoch)\n\n#         # Run validation at the end of every epochnbv\n#         run_validation(model, val_dataloader, config, device, global_step)\n\n#         # Save the model at the end of every epoch\n#         model_filename = get_weights_file_path(config, f\"{epoch:02d}\")\n#         torch.save({\n#             'epoch': epoch,\n#             'model_state_dict': model.state_dict(),\n#             'optimizer_state_dict': optimizer.state_dict(),\n#             'global_step': global_step\n#         }, model_filename)","metadata":{"execution":{"iopub.status.busy":"2024-09-01T13:27:08.872316Z","iopub.execute_input":"2024-09-01T13:27:08.872579Z","iopub.status.idle":"2024-09-01T13:27:08.887333Z","shell.execute_reply.started":"2024-09-01T13:27:08.872557Z","shell.execute_reply":"2024-09-01T13:27:08.886562Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"def run_validation_ensemble(models, validation_ds, config, device, global_step):\n    # Ensure the models are in evaluation mode\n    for model in models:\n        model.eval()\n\n    true_classes = []\n    pred_classes = []\n\n    with torch.no_grad():\n        i=0\n        for batch in validation_ds:\n            inp = batch[\"input\"].to(device)  # (b, d_model, seq_len)\n            inp_mask = None\n\n            # Check that the batch size is 1\n            assert inp.size(0) == 1, \"Batch size must be 1 for validation\"\n\n            # Aggregate the outputs from all models\n            outputs = torch.zeros((config['num_of_labels'],)).to(device)\n            model_predictions = []\n            for model in models:\n                out = model.encode(inp)\n                out = model.project(out).squeeze(0)\n                model_predictions.append(torch.argmax(out).item())\n            \n            \n            if i==0:\n                ot = torch.tensor(model_predictions)\n                print(ot.shape)\n                i+=1\n            # Average the outputs\n#             outputs /= len(models)\n            #getting majority votes\n            outputs = torch.tensor(model_predictions).mode().values.item()\n\n            # Store predictions and true labels\n#             pred_classes.append(torch.argmax(outputs).item())\n            pred_classes.append(outputs)\n            true_classes.append(torch.argmax(batch[\"label\"].squeeze(0)).item())\n\n        true_classes = torch.tensor(true_classes)\n        pred_classes = torch.tensor(pred_classes)\n\n    # Compute evaluation metrics\n    confusion_matrix = torchmetrics.ConfusionMatrix(num_classes=config[\"num_of_labels\"], task=\"multiclass\")\n    f1_score = torchmetrics.classification.MulticlassF1Score(num_classes=config[\"num_of_labels\"])\n    accuracy = torchmetrics.classification.MulticlassAccuracy(num_classes=config[\"num_of_labels\"])\n    recall = torchmetrics.classification.MulticlassRecall(num_classes=config[\"num_of_labels\"])\n    precision = torchmetrics.classification.MulticlassPrecision(num_classes=config[\"num_of_labels\"])\n\n    # Log metrics to WandB\n    wandb.log({\n        \"validation/confusion_matrix\": confusion_matrix(pred_classes, true_classes),\n        \"validation/accuracy\": accuracy(pred_classes, true_classes), \n        \"validation/recall\": recall(pred_classes, true_classes), \n        \"validation/precision\": precision(pred_classes, true_classes), \n        \"validation/f1_score\": f1_score(pred_classes, true_classes),\n        'global_step': global_step\n    })\n\n\ndef get_audio_location_list(df_location):\n    data = []\n    ref = pd.read_csv(df_location)\n    for index, row in ref.iterrows():\n        data.append([row['path'], row['labels']])\n\n    return data\n\ndef get_ds(config):\n    # It only has the train split, so we divide it overselves\n    ds_raw = get_audio_location_list(config['df_location'])\n    \n    # Keep 90% for training, 10% for validation\n    train_ds_raw_size = int(0.9 * len(ds_raw))\n    val_ds_raw_size = len(ds_raw) - train_ds_raw_size\n    train_ds_raw, val_ds_raw = random_split(ds_raw, [train_ds_raw_size, val_ds_raw_size])\n\n#     train_ds = AudioDataset(train_ds_raw, config[\"seq_len\"], config[\"d_model\"])\n#     val_ds = AudioDataset(val_ds_raw, config[\"seq_len\"], config[\"d_model\"])\n    train_ds = AudioDataset(train_ds_raw, config[\"seq_len\"], config[\"d_model\"],augment=True)\n    val_ds = AudioDataset(val_ds_raw, config[\"seq_len\"], config[\"d_model\"], augment=False)\n    \n#     print(train_ds_raw_size)\n#     print(len(train_ds))\n\n#     train_dataloader = DataLoader(train_ds, batch_size=config['batch_size'], shuffle=True)\n#     val_dataloader = DataLoader(val_ds, batch_size=1, shuffle=True)\n    train_dataloader = DataLoader(train_ds, batch_size=config['batch_size'], shuffle=True, num_workers=4, pin_memory=True)\n    \n    val_dataloader = DataLoader(val_ds, batch_size=1, shuffle=True, num_workers=4, pin_memory=True)\n\n    return train_dataloader, val_dataloader\n\n# def get_model(config):\n#     model = build_transformer( config[\"seq_len\"], config['num_of_labels'], d_model=config['d_model'])\n#     return model\ndef get_models(config,num_models=4):\n    models=[]\n    for _ in range(num_models):\n        model = build_transformer(config[\"seq_len\"], config['num_of_labels'], d_model=config['d_model'])\n        models.append(model)\n        \n    return models\n\ndef train_model(config, num_models=4):\n    # Define the device\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(\"Using device:\", device)\n\n    # Make sure the weights folder exists\n    Path(config['model_folder']).mkdir(parents=True, exist_ok=True)\n\n    train_dataloader, val_dataloader = get_ds(config)\n#     model = get_model(config).to(device)\n    models = get_models(config,num_models)\n    models = [model.to(device) for model in models]\n    \n#     optimizer = torch.optim.SGD(model.parameters(), lr=config['lr'])\n    optimizers = [torch.optim.SGD(model.parameters(), lr=config['lr']) for model in models]\n    schedulers = [SquareRootScheduler(lr=config['lr']) for _ in range(num_models)]\n#     scheduler = SquareRootScheduler(lr=config['lr'])\n\n    # If the user specified a model to preload before training, load it\n    initial_epoch = 0\n    global_step = 0\n    if config['preload']:\n        model_filename = latest_weights_file_path(config)\n        print(f'Preloading model {model_filename}')\n        state = torch.load(model_filename)\n        model.load_state_dict(state['model_state_dict'])\n        initial_epoch = state['epoch'] + 1\n        optimizer.load_state_dict(state['optimizer_state_dict'])\n        global_step = state['global_step']\n        del state\n\n    loss_fn = nn.CrossEntropyLoss(label_smoothing=0.1).to(device)\n\n    # define our custom x axis metric\n    wandb.define_metric(\"global_step\")\n    # define which metrics will be plotted against it\n    wandb.define_metric(\"validation/*\", step_metric=\"global_step\")\n    wandb.define_metric(\"train/*\", step_metric=\"global_step\")\n\n    for epoch in range(initial_epoch, initial_epoch+config['num_epochs']):\n        torch.cuda.empty_cache()\n        for model in models:\n            model.train()\n            \n#         model.train()\n        batch_iterator = tqdm(train_dataloader, desc=f\"Processing Epoch {epoch:02d}\")\n        for batch in batch_iterator:\n            for model, optimizer in zip(models, optimizers):\n                \n                # moved here\n                optimizer.zero_grad()\n\n                input = batch['input'].to(device) # (b, seq_len, d_model)\n\n                # Run the tensors through the encoder, decoder and the projection layer\n                encoder_output = model.encode(input) # (B, seq_len, d_model)\n                proj_output = model.project(encoder_output) # (B, num_of_labels)\n\n                # Compare the output with the label\n                label = batch['label'].to(device) # (B, num_of_labels)\n\n                # Compute the loss using a simple cross entropy\n                loss = loss_fn(proj_output, label)\n                batch_iterator.set_postfix({\"loss\": f\"{loss.item():6.3f}\"})\n\n                # Log the loss\n                wandb.log({'train/loss': loss.item(), 'global_step': global_step})\n\n                # Backpropagate the loss\n                loss.backward()\n\n                # Update the weights\n                optimizer.step()\n    #             optimizer.zero_grad(set_to_none=True)\n\n        global_step += 1\n        \n        # updating scheduler\n        for scheduler, optimizer in zip(schedulers, optimizers):\n            if scheduler:\n                if scheduler.__module__ == lr_scheduler.__name__:\n                    # Using PyTorch In-Built scheduler\n                    scheduler.step()\n                else:\n                    # Using custom defined scheduler\n                    for param_group in optimizer.param_groups:\n                        param_group['lr'] = scheduler(epoch)\n\n        # Run validation at the end of every epochnbv\n        run_validation_ensemble(models, val_dataloader, config, device, global_step)\n        \n        # Save the models at the end of every epoch\n        for i, model in enumerate(models):\n            model_filename = get_weights_file_path(config, f\"{epoch:02d}_model_{i}\")\n            torch.save({\n                'epoch': epoch,\n                'model_state_dict': model.state_dict(),\n                'optimizer_state_dict': optimizers[i].state_dict(),\n                'global_step': global_step\n            }, model_filename)","metadata":{"papermill":{"duration":0.043588,"end_time":"2024-05-23T21:04:55.027506","exception":false,"start_time":"2024-05-23T21:04:54.983918","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-09-01T13:34:03.157830Z","iopub.execute_input":"2024-09-01T13:34:03.158450Z","iopub.status.idle":"2024-09-01T13:34:03.190857Z","shell.execute_reply.started":"2024-09-01T13:34:03.158418Z","shell.execute_reply":"2024-09-01T13:34:03.190052Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"# #I have added this to check input data\n# config = get_config()\n# def get_audio_location_list(df_location):\n#     data = []\n#     ref = pd.read_csv(df_location)\n#     for index, row in ref.iterrows():\n#         data.append([row['path'], row['labels']])\n\n#     return data\n\n# def get_ds(config):\n#     # It only has the train split, so we divide it overselves\n#     ds_raw = get_audio_location_list(config['df_location'])\n    \n#     # Keep 90% for training, 10% for validation\n#     train_ds_raw_size = int(0.9 * len(ds_raw))\n#     val_ds_raw_size = len(ds_raw) - train_ds_raw_size\n#     train_ds_raw, val_ds_raw = random_split(ds_raw, [train_ds_raw_size, val_ds_raw_size])\n\n# #     train_ds = AudioDataset(train_ds_raw, config[\"seq_len\"], config[\"d_model\"])\n# #     val_ds = AudioDataset(val_ds_raw, config[\"seq_len\"], config[\"d_model\"])\n#     train_ds = AudioDataset(train_ds_raw, config[\"seq_len\"], config[\"d_model\"],augment=True)\n#     val_ds = AudioDataset(val_ds_raw, config[\"seq_len\"], config[\"d_model\"], augment=False)\n    \n#     print(train_ds_raw_size)\n#     print(len(train_ds))\n\n# #     train_dataloader = DataLoader(train_ds, batch_size=config['batch_size'], shuffle=True)\n# #     val_dataloader = DataLoader(val_ds, batch_size=1, shuffle=True)\n#     train_dataloader = DataLoader(train_ds, batch_size=config['batch_size'], shuffle=True, num_workers=4, pin_memory=True)\n    \n#     val_dataloader = DataLoader(val_ds, batch_size=1, shuffle=True, num_workers=4, pin_memory=True)\n\n#     return train_dataloader, val_dataloader\n\n\n\n# # Make sure the weights folder exists\n# Path(config['model_folder']).mkdir(parents=True, exist_ok=True)\n# train_dataloader, val_dataloader = get_ds(config)\n# initial_epoch = 0\n# global_step = 0\n# for epoch in range(initial_epoch, initial_epoch+config['num_epochs']):\n#         torch.cuda.empty_cache()\n#         batch_iterator = tqdm(train_dataloader, desc=f\"Processing Epoch {epoch:02d}\")\n#         for batch in batch_iterator:\n#             input = batch['input'] # (b, seq_len, d_model)\n# #             print(input.shape) #torch.Size([16, 600, 39])","metadata":{"execution":{"iopub.status.busy":"2024-09-01T13:27:08.922718Z","iopub.execute_input":"2024-09-01T13:27:08.923037Z","iopub.status.idle":"2024-09-01T13:27:08.936051Z","shell.execute_reply.started":"2024-09-01T13:27:08.923008Z","shell.execute_reply":"2024-09-01T13:27:08.935176Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"os.mkdir(\"Global_dataset_ravdess_tess_weights\")                         ","metadata":{"papermill":{"duration":0.016078,"end_time":"2024-05-23T21:04:55.051212","exception":false,"start_time":"2024-05-23T21:04:55.035134","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-09-01T13:27:08.937095Z","iopub.execute_input":"2024-09-01T13:27:08.937429Z","iopub.status.idle":"2024-09-01T13:27:08.950186Z","shell.execute_reply.started":"2024-09-01T13:27:08.937398Z","shell.execute_reply":"2024-09-01T13:27:08.949330Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# !pip install wandb\n# !wandb login\n# WANDB_API_KEY = \"5917e81ca177aa743d512c104ea62b82dc6da9f5\"\n","metadata":{"papermill":{"duration":4.486513,"end_time":"2024-05-24T01:27:03.371812","exception":false,"start_time":"2024-05-24T01:26:58.885299","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-09-01T13:28:57.731569Z","iopub.status.idle":"2024-09-01T13:28:57.731903Z","shell.execute_reply.started":"2024-09-01T13:28:57.731742Z","shell.execute_reply":"2024-09-01T13:28:57.731756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# wandb login --relogin                 v          x                                                                                                                                      v                                                                                                                v                        v                                              v                                                                  b v           v                                                                             0                     x                   x                                                                z                                                  x         ","metadata":{"papermill":{"duration":4.41924,"end_time":"2024-05-24T01:26:54.363684","exception":false,"start_time":"2024-05-24T01:26:49.944444","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-09-01T13:28:57.729809Z","iopub.status.idle":"2024-09-01T13:28:57.730197Z","shell.execute_reply.started":"2024-09-01T13:28:57.730020Z","shell.execute_reply":"2024-09-01T13:28:57.730036Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}}]}